{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Traveling Salesman Problem (DTSP) with RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TSP Environment using Gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from copy import copy, deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create lists to store progress data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_log = []\n",
    "dist_log = []\n",
    "solved_log = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the TSPEnv environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a Gymnasium environment to simulate the traveling salesman problem.\n",
    "\n",
    "We define the observation space of the environment to be a one-dimensional list with the following elements:\n",
    "- The current node (0 -> n-1)\n",
    "- Whether each node has been visited\n",
    "- A partial distance matrix between the nodes\n",
    "\n",
    "The actor can input an integer (0 -> n-1) to move to the corresponding node. This is the action space.\n",
    "\n",
    "The reward function is as follows:\n",
    "- Each move cost is proportional to the distance traveled divided by the minimum possible path. This is to normalize between random generations of nodes\n",
    "- Traveling to the current node (\"stalling\") is penalized with 100 points\n",
    "- Traveling to an already visited node (\"repeating\") is penalized with 100 points\n",
    "- Traveling to an unvisited node is rewarded with 100 points\n",
    "- Visiting every node is rewarded with 1000 points\n",
    "- The reward is deducted by a value proportional to the ratio between the chosen path and the shortest possible path\n",
    "- A perfect path is rewarded with an additional 1000 points\n",
    "\n",
    "When this is extended, it is likely that we need to remove the requirement to have already found the shortest possible path algorithmically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSPEnv(gym.Env):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.N = 7 # Number of nodes\n",
    "        self.dist_factor = 20 # Multiplier for cost of movement\n",
    "        self.stall_cost = -100 # Penalty for moving to the current node\n",
    "        self.repeat_cost = -100 # Penalty for re-visiting a node\n",
    "        self.new_reward = 100 # Reward for visiting a new node\n",
    "        # Threshold for stopping training\n",
    "        self.spec = SimpleNamespace(reward_threshold=1000)\n",
    "\n",
    "        # Define the observation space\n",
    "        # [current node] + [node visited? for each node] + [distance matrix (0-200)]\n",
    "        self.observation_space = spaces.MultiDiscrete(\n",
    "            [self.N+1] + [2]*self.N + [200]*(self.N*(self.N-1)//2)\n",
    "        )\n",
    "        # Define the action space... allow movement to any node\n",
    "        self.action_space = spaces.Discrete(self.N)\n",
    "\n",
    "        self.locations = []\n",
    "        self.min_dist = -1\n",
    "        self.step_count = 0\n",
    "        self.nodes = np.arange(self.N)\n",
    "        self.step_limit = 2*self.N\n",
    "        self.render_ready = False\n",
    "        self.path = []\n",
    "        self.dist_sum = 0\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        self.path.append(action)\n",
    "        dist = self._node_dist(self.current_node, action) / self.min_dist\n",
    "        self.dist_sum += dist\n",
    "        reward = -dist * self.dist_factor\n",
    "        if action == self.current_node:\n",
    "            reward -= 100\n",
    "        if self.visit_log[action] >= 1:\n",
    "            reward -= 100\n",
    "        self.current_node = action\n",
    "        if self.visit_log[self.current_node] == 0:\n",
    "            reward += 100\n",
    "        self.visit_log[self.current_node] += 1\n",
    "            \n",
    "        self.state = self._update_state()\n",
    "        self.step_count += 1\n",
    "        # See if all nodes have been visited\n",
    "        unique_visits = sum([1 if v > 0 else 0 \n",
    "            for v in self.visit_log.values()])\n",
    "        if unique_visits >= self.N:\n",
    "            self.path.append(self.path[0])\n",
    "            # dist = np.sqrt(self._node_sqdist(self.current_node, action))\n",
    "            dist = np.sqrt(self._node_sqdist(self.current_node, action)) / self.min_dist\n",
    "            self.dist_sum += dist\n",
    "            reward -= dist * self.dist_factor\n",
    "            done = True\n",
    "            reward += 1000 - 500 * (self.dist_sum / self.min_dist)\n",
    "            if self.dist_sum <= self.min_dist * 1.05:\n",
    "                reward += 1000>= self.step_limit:\n",
    "            done = True\n",
    "            \n",
    "        if done and self.render_ready:\n",
    "            print(f\"Locations: {self.locations}\")\n",
    "            print(f\"Path: {self.path}\")\n",
    "            fig, ax = plt.subplots(figsize=(12,8))\n",
    "            for n in range(self.N):\n",
    "                pt = self.locations[n]\n",
    "                clr = 'green' if n == 0 else 'black'\n",
    "                ax.scatter(pt[0], pt[1], color=clr, s=300)\n",
    "                ax.annotate(r\"$N_{:d}$\".format(n), xy=(pt[0]+0.4, pt[1]+0.05), zorder=2)\n",
    "            for i in range(len(self.path) - 1):\n",
    "                ax.plot([self.locations[self.path[i]][0], self.locations[self.path[i+1]][0]],\n",
    "                    [self.locations[self.path[i]][1], self.locations[self.path[i+1]][1]], 'bo', linestyle='solid')\n",
    "            current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            fig.savefig(f\"{self.N}-solution-{current_datetime}.png\")\n",
    "        return self.state, reward, done, (self.step_count >= self.step_limit), {}\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if self.step_count > 0:\n",
    "            steps_log.append(self.step_count)\n",
    "            if self.step_count < 10:\n",
    "                solved_log.append(1)\n",
    "                dist_log.append(self.dist_sum)\n",
    "            else:\n",
    "                dist_log.append(1000)\n",
    "                solved_log.append(0)\n",
    "        self.step_count = 0\n",
    "        self.dist_sum = 0\n",
    "        self.current_node = np.random.choice(self.nodes)\n",
    "        self.visit_log = {n: 0 for n in self.nodes}\n",
    "        self.visit_log[self.current_node] += 1\n",
    "        self._generate_locations()\n",
    "        self.path = [self.current_node]\n",
    "        self.state = self._update_state()\n",
    "        return self.state, {}\n",
    "\n",
    "    def _update_state(self):\n",
    "        visit_list = [min(self.visit_log[i], 1) for i in range(self.N)]\n",
    "        dist_matrix = self.generate_1d_distance_matrix()\n",
    "        state = np.array([self.current_node] + visit_list + dist_matrix)\n",
    "        # print(f'state: {state}')\n",
    "        return state\n",
    "    \n",
    "    def _node_sqdist(self, a, b):\n",
    "        apt = self.locations[a]\n",
    "        bpt = self.locations[b]\n",
    "        dx = apt[0] - bpt[0]\n",
    "        dy = apt[1] - bpt[1]\n",
    "        return dx*dx + dy*dy\n",
    "    \n",
    "    def _node_dist(self, a, b):\n",
    "        return np.sqrt(self._node_sqdist(a, b))\n",
    "\n",
    "    def _node_dist_manhattan(self, a, b):\n",
    "        apt = self.locations[a]\n",
    "        bpt = self.locations[b]\n",
    "        dx = apt[0] - bpt[0]\n",
    "        dy = apt[1] - bpt[1]\n",
    "        return abs(dx) + abs(dy)\n",
    "\n",
    "    def _generate_locations(self):\n",
    "        self.locations.clear()\n",
    "        for i in range(self.N):\n",
    "            self.locations.append((np.random.randint(0,100), np.random.randint(0,100)))\n",
    "        self.min_dist = self.find_min_dist([self.current_node])\n",
    "\n",
    "    def generate_1d_distance_matrix(self):\n",
    "        matrix = []\n",
    "        for i in range(self.N):\n",
    "            for j in range(self.N):\n",
    "                matrix.append(self._node_dist_manhattan(i, j))\n",
    "        return matrix\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.render_ready:\n",
    "            print(f\"moving to {action}\")\n",
    "        return self._STEP(action)\n",
    "\n",
    "    def render(self):\n",
    "        if not self.render_ready:\n",
    "            self.render_ready = True\n",
    "\n",
    "    # Recursive DFS brute force algorithm for training\n",
    "    def find_min_dist(self, arr):\n",
    "        low = 9999999\n",
    "        low_i = -1\n",
    "        unique = 0\n",
    "        for i in range(self.N):\n",
    "            if arr.count(i) == 0:\n",
    "                md = self.find_min_dist(arr + [i]) + self._node_dist_manhattan(arr[-1], i) #np.sqrt(self._node_sqdist(arr[-1], i))\n",
    "                if md < low:\n",
    "                    low = md\n",
    "                    low_i = i\n",
    "            else:\n",
    "                unique += 1\n",
    "        if unique == self.N:\n",
    "            return self._node_dist_manhattan(arr[-1], arr[0])\n",
    "        return low\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function for saving graphs for progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_log(data, name):\n",
    "    if len(data) == 0:\n",
    "        print(f'{name} data len is 0')\n",
    "        return\n",
    "    avgs = []\n",
    "    num_pts = 100\n",
    "    for i in range(num_pts):\n",
    "        sum = 0\n",
    "        for i in range(len(data)//num_pts * i, len(data)//num_pts * (i+1)):\n",
    "            sum += data[i]\n",
    "        avgs.append(sum / (len(data) // num_pts))\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(range(num_pts), avgs)\n",
    "    fig.savefig(f'{data}_log.png')\n",
    "\n",
    "def save_logs():\n",
    "    save_log(steps_log, 'steps')\n",
    "    save_log(dist_log, 'dist')\n",
    "    save_log(solved_log, 'solved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model to train based on the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import a reinforcement learning library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tianshou as ts\n",
    "import torch, numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register the environment with Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.register(\n",
    "     id='TSPEnv-v0',\n",
    "     entry_point=TSPEnv,\n",
    "     max_episode_steps=50,\n",
    "     kwargs={},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create environments for training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_envs = ts.env.SubprocVectorEnv([lambda: gym.make('TSPEnv-v0') for _ in range(10)])\n",
    "test_envs = ts.env.SubprocVectorEnv([lambda: gym.make('TSPEnv-v0') for _ in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_shape), 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, np.prod(action_shape)),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "        batch = obs.shape[0]\n",
    "        logits = self.model(obs.view(batch, -1))\n",
    "        return logits, state\n",
    "\n",
    "state_shape = env.observation_space.shape or env.observation_space.n\n",
    "print(f'State shape: {state_shape}')\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "net = Net(state_shape, action_shape)\n",
    "optim = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "policy = ts.policy.DQNPolicy(\n",
    "    model=net,\n",
    "    optim=optim,\n",
    "    action_space=env.action_space,\n",
    "    discount_factor=0.9,\n",
    "    estimation_step=3,\n",
    "    target_update_freq=320\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create collectors to execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(20000, 10), exploration_noise=True)\n",
    "test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ts.trainer.OffpolicyTrainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=50, \n",
    "    step_per_epoch=10000,\n",
    "    step_per_collect=10,\n",
    "    update_per_step=0.1, episode_per_test=100, batch_size=64,\n",
    "    train_fn=lambda epoch, env_step: policy.set_eps(0.1),\n",
    "    test_fn=lambda epoch, env_step: policy.set_eps(0.05),\n",
    "    stop_fn=lambda mean_rewards: mean_rewards >= env.spec.reward_threshold\n",
    ").run()\n",
    "print(f'Finished training! Took {result[\"duration\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create examples and save logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.eval()\n",
    "policy.set_eps(0.05)\n",
    "collector = ts.data.Collector(policy, env, exploration_noise=True)\n",
    "collector.collect(n_episode=5, render=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
